{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592324fe",
   "metadata": {},
   "source": [
    "Q1\n",
    "Bayes' theorem, named after Thomas Bayes, is a fundamental concept in probability theory and statistics. It describes how to update the probability of a hypothesis or event based on new evidence or information. The theorem is mathematically expressed as:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A given event B has occurred.\n",
    "P(B|A) is the conditional probability of event B given event A has occurred.\n",
    "P(A) and P(B) are the probabilities of events A and B, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fe8af",
   "metadata": {},
   "source": [
    "Q2\n",
    "The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A given event B has occurred.\n",
    "P(B|A) is the conditional probability of event B given event A has occurred.\n",
    "P(A) and P(B) are the probabilities of events A and B, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441acf2",
   "metadata": {},
   "source": [
    "Q3\n",
    "Medical Diagnostics: Bayes' theorem is employed in medical diagnostics to calculate the probability of a patient having a particular condition based on observed symptoms or test results. It combines prior probabilities (prevalence of the condition) with the likelihoods of specific symptoms or test outcomes to provide a more accurate diagnosis.\n",
    "\n",
    "Spam Filtering: Email spam filters often utilize Bayes' theorem to classify incoming emails as either spam or legitimate. The algorithm assigns probabilities to different words or features in an email and updates these probabilities based on known spam or non-spam training data, allowing it to make predictions on new emails.\n",
    "\n",
    "Machine Learning: Bayes' theorem serves as a foundation for Bayesian machine learning methods. It is used in Bayesian networks and Bayesian inference algorithms to update beliefs about model parameters or hypotheses as new data is observed.\n",
    "\n",
    "Risk Assessment: Bayes' theorem is employed in risk assessment and decision-making processes. It allows for the incorporation of prior knowledge and expert judgments into the assessment of risks associated with certain events or actions.\n",
    "\n",
    "A/B Testing: In the field of online experimentation, Bayes' theorem can be used to analyze A/B tests. By updating prior beliefs about the effectiveness of different versions of a web page or application with the observed conversion rates, it helps determine the probability that one version is better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff6ce6",
   "metadata": {},
   "source": [
    "Q4\n",
    "Bayes' theorem provides a mathematical relationship between conditional probabilities. It allows us to calculate the conditional probability of an event A given that event B has occurred, by utilizing the conditional probability of event B given event A and the individual probabilities of events A and B.\n",
    "\n",
    "Let's denote event A as a hypothesis or an event of interest, and event B as the observed evidence. The formula for Bayes' theorem is:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "In this formula:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A given event B has occurred.\n",
    "P(B|A) is the conditional probability of event B given event A has occurred.\n",
    "P(A) and P(B) are the probabilities of events A and B, respectively.\n",
    "Bayes' theorem allows us to update our prior belief or probability of event A based on new evidence B. It combines the likelihood of observing evidence B if the hypothesis A is true (P(B|A)) with the prior probability of A (P(A)), and normalizes it by dividing by the overall probability of observing B (P(B)).\n",
    "\n",
    "In summary, Bayes' theorem relates conditional probabilities by providing a framework to update probabilities based on new information, allowing us to make more accurate assessments or predictions. It combines our prior beliefs with observed evidence, incorporating uncertainty into the analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec98b29",
   "metadata": {},
   "source": [
    "Q5\n",
    "Gaussian Naive Bayes (GNB): GNB assumes that the features follow a Gaussian (normal) distribution. It works well when the continuous features in your data can be reasonably approximated by a Gaussian distribution. It is suitable for problems with continuous features.\n",
    "\n",
    "Multinomial Naive Bayes (MNB): MNB is appropriate when dealing with discrete features, particularly when the data is represented as word frequencies or categorical counts. It is commonly used in text classification tasks, such as spam detection or document classification.\n",
    "\n",
    "Bernoulli Naive Bayes (BNB): BNB is similar to MNB but assumes binary features (0s and 1s) instead of frequency counts. It works well when the features are binary or when you want to model presence or absence of features. Like MNB, it is commonly used in text classification problems.\n",
    "\n",
    "Choosing the most suitable variant depends on the nature of your data and the assumptions that best align with your problem. Here are some guidelines to help you make a decision:\n",
    "\n",
    "Data type: Consider whether your features are continuous, discrete, or binary. GNB is suitable for continuous features, MNB for discrete features, and BNB for binary features.\n",
    "\n",
    "Feature independence assumption: Naive Bayes classifiers assume that features are independent of each other given the class label. If this assumption is reasonable for your problem, Naive Bayes can work well. However, if the independence assumption is strongly violated, other classifiers may be more appropriate.\n",
    "\n",
    "Availability of labeled data: The choice of classifier can also depend on the amount of labeled training data available. If you have a small dataset, Naive Bayes classifiers tend to work well due to their simplicity and ability to handle high-dimensional data efficiently.\n",
    "\n",
    "Prior knowledge: Consider any prior knowledge or domain expertise you have about the problem. Some variants may align better with the underlying distribution of your data based on your understanding of the problem.\n",
    "\n",
    "It is worth noting that Naive Bayes classifiers are generally fast, easy to implement, and have low computational requirements. Therefore, it can be beneficial to try different variants and compare their performance using cross-validation or other evaluation techniques specific to your problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5f1a9",
   "metadata": {},
   "source": [
    "Q6\n",
    "To determine which class the Naive Bayes classifier would predict for the new instance with features X1=3 and X2=4, we need to calculate the conditional probabilities for each class and select the class with the highest probability.\n",
    "\n",
    "Given the table of feature frequencies for each class, we can compute the conditional probabilities as follows:\n",
    "\n",
    "P(A) = P(B) = 1/2 (equal prior probabilities for each class)\n",
    "\n",
    "P(X1=3|A) = 4/13\n",
    "P(X1=3|B) = 1/7\n",
    "\n",
    "P(X2=4|A) = 3/13\n",
    "P(X2=4|B) = 3/7\n",
    "\n",
    "To calculate the probability of the new instance belonging to class A, we multiply the conditional probabilities for each feature given class A:\n",
    "\n",
    "P(A|X1=3, X2=4) = P(X1=3|A) * P(X2=4|A) * P(A)\n",
    "\n",
    "P(A|X1=3, X2=4) = (4/13) * (3/13) * (1/2) = 12/338 ≈ 0.0355\n",
    "\n",
    "Similarly, we calculate the probability of the new instance belonging to class B:\n",
    "\n",
    "P(B|X1=3, X2=4) = P(X1=3|B) * P(X2=4|B) * P(B)\n",
    "\n",
    "P(B|X1=3, X2=4) = (1/7) * (3/7) * (1/2) = 3/98 ≈ 0.0306\n",
    "\n",
    "Comparing the probabilities, we see that P(A|X1=3, X2=4) > P(B|X1=3, X2=4). Therefore, according to the Naive Bayes classifier, the new instance with features X1=3 and X2=4 would be predicted to belong to class A.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ec977",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
